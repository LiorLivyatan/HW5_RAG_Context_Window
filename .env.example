# Context Windows Lab - Environment Variables
# Copy this file to .env and update with your settings

# ============================================================================
# Ollama Configuration
# ============================================================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_TEMPERATURE=0.0
OLLAMA_MAX_TOKENS=500
OLLAMA_TIMEOUT=60

# ============================================================================
# Embedding Configuration
# ============================================================================
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSION=768

# ============================================================================
# ChromaDB Configuration
# ============================================================================
CHROMA_PERSIST_DIR=./.chroma
CHROMA_COLLECTION_NAME=experiment_3_docs

# ============================================================================
# Experiment Configuration
# ============================================================================
# Set to false to disable parallelization for debugging
USE_MULTIPROCESSING=true
USE_MULTITHREADING=true

# Maximum number of processes/threads (null = auto-detect CPU count)
MAX_PROCESSES=
MAX_THREADS_PER_PROCESS=5

# Random seed for reproducibility (optional, null = random)
RANDOM_SEED=42

# ============================================================================
# Logging Configuration
# ============================================================================
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=logs/context_windows_lab.log
LOG_TO_CONSOLE=true
LOG_TO_FILE=true

# ============================================================================
# Output Configuration
# ============================================================================
RESULTS_DIR=./results
DATA_DIR=./data
OUTPUT_FORMAT=png  # Options: png, pdf, svg

# ============================================================================
# Performance Configuration
# ============================================================================
# Enable result caching to avoid redundant LLM calls
ENABLE_CACHING=true
CACHE_DIR=./.cache

# ============================================================================
# Development/Debug Settings
# ============================================================================
# Disable parallelization for easier debugging
DEBUG_MODE=false

# Run experiments with fewer iterations for faster testing
DEBUG_ITERATIONS=1

# Save intermediate results for inspection
SAVE_INTERMEDIATE_RESULTS=false

# ============================================================================
# Future: External API Keys (if migrating to cloud LLMs)
# ============================================================================
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ============================================================================
# Notes
# ============================================================================
# - This file should NOT be committed to git (.gitignore should exclude .env)
# - Copy this file to .env and customize for your local setup
# - All values have sensible defaults in the code if not specified
# - Environment variables override config files (config/llm_config.yaml, etc.)
